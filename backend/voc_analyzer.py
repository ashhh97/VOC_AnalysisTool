import openpyxl
import requests
import json
import re
import os
import math
import pandas as pd
from datetime import datetime
from openpyxl import load_workbook
from openpyxl.utils import get_column_letter

# å°è¯•å¯¼å…¥é…ç½®æ–‡ä»¶
try:
    from config import HF_API_TOKEN, TONGYI_API_KEY, TONGYI_MODEL, API_PRIORITY
except ImportError:
    # å¦‚æœé…ç½®æ–‡ä»¶ä¸å­˜åœ¨ï¼Œä½¿ç”¨é»˜è®¤å€¼
    HF_API_TOKEN = None
    TONGYI_API_KEY = None
    TONGYI_MODEL = "qwen-turbo"
    API_PRIORITY = ["hf_token", "tongyi", "hf_free", "local"]

class VOCAnalyzer:
    def __init__(self):
        # åŠ è½½APIé…ç½®
        self.hf_token = HF_API_TOKEN or os.getenv('HF_API_TOKEN')
        self.tongyi_key = TONGYI_API_KEY or os.getenv('TONGYI_API_KEY')
        # è·å–æ¨¡å‹åç§°ï¼Œå¦‚æœæœªé…ç½®åˆ™ä½¿ç”¨é»˜è®¤å€¼
        self.tongyi_model = TONGYI_MODEL if 'TONGYI_MODEL' in globals() else (os.getenv('TONGYI_MODEL') or "qwen-turbo")
        self.api_priority = API_PRIORITY
        
        # Hugging Face APIç«¯ç‚¹ï¼ˆä½¿ç”¨Tokenï¼‰
        self.hf_api_urls = [
            "https://api-inference.huggingface.co/models/Qwen/Qwen2.5-7B-Instruct",
            "https://api-inference.huggingface.co/models/Qwen/Qwen2-7B-Instruct",
            "https://api-inference.huggingface.co/models/Qwen/Qwen2-1.5B-Instruct",
            "https://api-inference.huggingface.co/models/Qwen/Qwen2.5-14B-Instruct",
        ]
        
        # Hugging Faceå…è´¹APIç«¯ç‚¹ï¼ˆæ— éœ€Tokenï¼Œä½†å¯èƒ½ä¸å¯ç”¨ï¼‰
        self.hf_free_api_urls = [
            "https://api-inference.huggingface.co/models/Qwen/Qwen2.5-7B-Instruct",
            "https://api-inference.huggingface.co/models/Qwen/Qwen2-7B-Instruct",
            "https://api-inference.huggingface.co/models/Qwen/Qwen2-1.5B-Instruct",
        ]
        
        # é€šä¹‰åƒé—®APIç«¯ç‚¹
        self.tongyi_api_url = "https://dashscope.aliyuncs.com/api/v1/services/aigc/text-generation/generation"
        
        self.current_api_index = 0
        self.use_local_analysis = False
        self.stop_flag = None
        
        # æ‰“å°é…ç½®ä¿¡æ¯
        print(f"[VOC Analyzer] åˆå§‹åŒ–å®Œæˆ")
        if self.hf_token:
            print(f"[VOC Analyzer] Hugging Face Tokenå·²é…ç½®")
        if self.tongyi_key:
            print(f"[VOC Analyzer] é€šä¹‰åƒé—®API Keyå·²é…ç½®ï¼Œæ¨¡å‹: {self.tongyi_model}")
        print(f"[VOC Analyzer] APIä¼˜å…ˆçº§: {', '.join(self.api_priority)}")
    
    def set_stop_flag(self, stop_flag):
        """è®¾ç½®åœæ­¢æ ‡å¿—"""
        self.stop_flag = stop_flag
    
    def analyze_with_ai(self, text):
        """ä½¿ç”¨Qwen AIåˆ†ææ–‡æœ¬æƒ…æ„Ÿå’Œåˆ†ç±»ï¼ŒæŒ‰ä¼˜å…ˆçº§å°è¯•ä¸åŒçš„API"""
        if self.use_local_analysis:
            return self.local_analyze(text)
        
        # æ„é€ prompt
        prompt = f"""Role (è§’è‰²è®¾å®š):
ä½ æ˜¯ä¸€åæ‹¥æœ‰10å¹´ç»éªŒçš„ B2B SaaS äº§å“ä½“éªŒåˆ†æå¸ˆã€‚ä½ çš„ä»»åŠ¡æ˜¯æ¸…æ´—ç”¨æˆ·åé¦ˆæ•°æ®ï¼ˆVOCï¼‰ï¼Œç²¾å‡†è¯†åˆ«ç”¨æˆ·ç—›ç‚¹ï¼Œå¹¶è¿›è¡Œæ ‡å‡†åŒ–çš„åˆ†ç±»å½’çº³ã€‚

Critical Rules (æ ¸å¿ƒåˆ¤åˆ«è§„åˆ™ - å¿…é¡»ä¸¥æ ¼éµå®ˆ):
1. Bug vs. çµæ´»æ€§ (æœ€é«˜ä¼˜å…ˆçº§):
   - åˆ¤å®šä¸º [åŠŸèƒ½ - Bug/ç¨³å®šæ€§]ï¼šå½“ç”¨æˆ·æè¿°"æ“ä½œæ— æ•ˆ"ã€"æŠ¥é”™"ã€"æ˜¾ç¤ºå¼‚å¸¸"ã€"æ­»æœº"ã€"æ˜æ˜è®¾ç½®äº†ä½†æ²¡ååº”"ç­‰é¢„æœŸåŠŸèƒ½å¤±æ•ˆçš„æƒ…å†µã€‚
   - åˆ¤å®šä¸º [åŠŸèƒ½ - çµæ´»æ€§/é…ç½®èƒ½åŠ›]ï¼šåªæœ‰å½“ç”¨æˆ·æ˜ç¡®è¡¨ç¤º"å¸Œæœ›èƒ½è‡ªå®šä¹‰..."ã€"æƒ³è¦æ”¯æŒ...åŠŸèƒ½"ã€"ç›®å‰é€‰é¡¹å¤ªå°‘"ç­‰æ–°å¢éœ€æ±‚æ—¶ã€‚
   - æ¡ˆä¾‹ï¼š"ä¸»é¡µæ¿å—åŠ é“¾æ¥åå›¾ç‰‡ä¸æ˜¾ç¤º" -> [åŠŸèƒ½ - Bug/ç¨³å®šæ€§]ã€‚

2. æ¦‚æ‹¬åº¦æ§åˆ¶ (å½’çº³æ³•):
   - å°†ç›¸ä¼¼çš„å…·ä½“é—®é¢˜å‘ä¸Šå½’çº³åˆ°çˆ¶ç±»ç›®ã€‚
   - æ¡ˆä¾‹ï¼š"æ–°æ‰‹æ•™ç¨‹ç¼ºå¤±"ã€"å¼€å‘æ–‡æ¡£ä¸å…¨" -> [æœåŠ¡ - å¸®åŠ©ä¸å¼•å¯¼]ã€‚

Taxonomy (æ ‡å‡†åŒ–åˆ†ç±»ä½“ç³» - è¯·ä»…ä»ä»¥ä¸‹åˆ—è¡¨ä¸­é€‰æ‹©):
- åŠŸèƒ½ - Bug/ç¨³å®šæ€§
- åŠŸèƒ½ - çµæ´»æ€§/é…ç½®èƒ½åŠ›
- åŠŸèƒ½ - å®ç”¨æ€§/å®Œæ•´åº¦
- ä½“éªŒ - æ“ä½œå¤æ‚åº¦
- ä½“éªŒ - æ€§èƒ½/åŠ è½½é€Ÿåº¦
- èµ„æº - æ¨¡æ¿ä¸°å¯Œåº¦
- èµ„æº - æ’ä»¶ç”Ÿæ€
- æœåŠ¡ - å¸®åŠ©ä¸å¼•å¯¼

è¯·åˆ†æä»¥ä¸‹ç”¨æˆ·åé¦ˆï¼Œè¿”å›ä¸€ä¸ªJSONå¯¹è±¡ï¼š
{{
    "category": "å¿…é¡»ä»ä¸Šæ–¹Taxonomyåˆ—è¡¨ä¸­é€‰æ‹©ä¸€ä¸ªæ ‡å‡†çš„åˆ†ç±»åç§° (ä¾‹å¦‚: åŠŸèƒ½ - Bug/ç¨³å®šæ€§)",
    "sentiment": "æ­£é¢ğŸ˜Š/è´Ÿé¢ğŸ˜ /ä¸­æ€§ğŸ˜",
    "rationale": "ç®€çŸ­çš„åˆ†ç±»ç†ç”±"
}}

ç”¨æˆ·åé¦ˆï¼š{text}

è¯·åªè¿”å›å•ä¸ªJSONå¯¹è±¡ï¼š"""
        
        # æŒ‰ä¼˜å…ˆçº§å°è¯•ä¸åŒçš„API
        for api_type in self.api_priority:
            if api_type == "hf_token" and self.hf_token:
                result = self._try_huggingface_token(prompt, text)
                if result:
                    return result
            elif api_type == "tongyi" and self.tongyi_key:
                result = self._try_tongyi_api(prompt, text)
                if result:
                    return result
            elif api_type == "hf_free":
                result = self._try_huggingface_free(prompt, text)
                if result:
                    return result
            elif api_type == "local":
                print("[Qwen API] ä½¿ç”¨æœ¬åœ°åˆ†æ")
                return self.local_analyze(text)
        
        # æ‰€æœ‰APIéƒ½å¤±è´¥ï¼Œä½¿ç”¨æœ¬åœ°åˆ†æ
        print("[Qwen API] æ‰€æœ‰APIéƒ½ä¸å¯ç”¨ï¼Œä½¿ç”¨æœ¬åœ°åˆ†æ")
        return self.local_analyze(text)
    
    def _try_huggingface_token(self, prompt, text):
        """å°è¯•ä½¿ç”¨Hugging Face API Token"""
        for api_url in self.hf_api_urls:
            try:
                headers = {
                    "Content-Type": "application/json",
                    "Authorization": f"Bearer {self.hf_token}"
                }
                payload = {
                    "inputs": prompt,
                    "parameters": {
                        "max_new_tokens": 150,
                        "temperature": 0.3,
                        "return_full_text": False
                    }
                }
                
                print(f"[HF Token API] å°è¯•è°ƒç”¨: {api_url}")
                response = requests.post(api_url, headers=headers, json=payload, timeout=30)
                
                if response.status_code == 200:
                    result = response.json()
                    print(f"[HF Token API] è°ƒç”¨æˆåŠŸ")
                    return self.parse_ai_result(result, text)
                elif response.status_code == 503:
                    error_info = response.json() if response.content else {}
                    estimated_time = error_info.get('estimated_time', 0)
                    print(f"[HF Token API] æ¨¡å‹æ­£åœ¨åŠ è½½ï¼Œé¢„è®¡ç­‰å¾…æ—¶é—´: {estimated_time}ç§’")
                    if estimated_time and estimated_time < 30:
                        import time
                        time.sleep(min(estimated_time + 2, 30))
                        retry_response = requests.post(api_url, headers=headers, json=payload, timeout=30)
                        if retry_response.status_code == 200:
                            return self.parse_ai_result(retry_response.json(), text)
                    continue
                else:
                    print(f"[HF Token API] é”™è¯¯ {response.status_code}: {response.text[:200]}")
                    continue
            except Exception as e:
                print(f"[HF Token API] è°ƒç”¨å¤±è´¥: {e}")
                continue
        return None
    
    def _try_tongyi_api(self, prompt, text):
        """å°è¯•ä½¿ç”¨é€šä¹‰åƒé—®API"""
        try:
            headers = {
                "Content-Type": "application/json",
                "Authorization": f"Bearer {self.tongyi_key}"
            }
            payload = {
                "model": self.tongyi_model,  # ä½¿ç”¨é…ç½®çš„æ¨¡å‹
                "input": {
                    "messages": [
                        {
                            "role": "user",
                            "content": prompt
                        }
                    ]
                },
                "parameters": {
                    "max_tokens": 150,
                    "temperature": 0.3
                }
            }
            
            print(f"[é€šä¹‰åƒé—®API] å°è¯•è°ƒç”¨æ¨¡å‹: {self.tongyi_model}")
            response = requests.post(self.tongyi_api_url, headers=headers, json=payload, timeout=30)
            
            if response.status_code == 200:
                result = response.json()
                print(f"[é€šä¹‰åƒé—®API] å“åº”çŠ¶æ€: 200")
                
                # é€šä¹‰åƒé—®APIçš„å“åº”æ ¼å¼å¯èƒ½æ˜¯ä¸¤ç§ï¼š
                # 1. æ–°æ ¼å¼: result['output']['text'] ç›´æ¥åŒ…å«æ–‡æœ¬
                # 2. æ—§æ ¼å¼: result['output']['choices'][0]['message']['content']
                generated_text = None
                
                if result.get('output'):
                    output = result['output']
                    # å°è¯•æ–°æ ¼å¼ï¼ˆtextå­—æ®µï¼‰
                    if 'text' in output:
                        generated_text = output['text']
                        print(f"[é€šä¹‰åƒé—®API] ä½¿ç”¨textå­—æ®µè·å–ç»“æœ")
                    # å°è¯•æ—§æ ¼å¼ï¼ˆchoiceså­—æ®µï¼‰
                    elif 'choices' in output and len(output['choices']) > 0:
                        generated_text = output['choices'][0]['message']['content']
                        print(f"[é€šä¹‰åƒé—®API] ä½¿ç”¨choiceså­—æ®µè·å–ç»“æœ")
                
                if generated_text:
                    print(f"[é€šä¹‰åƒé—®API] è°ƒç”¨æˆåŠŸï¼Œè¿”å›æ–‡æœ¬é•¿åº¦: {len(generated_text)}")
                    # è§£æç»“æœ
                    return self.parse_ai_result({'generated_text': generated_text}, text)
                else:
                    print(f"[é€šä¹‰åƒé—®API] å“åº”æ ¼å¼å¼‚å¸¸ï¼Œæœªæ‰¾åˆ°textæˆ–choices: {result}")
                    return None
            elif response.status_code == 429:
                # é€Ÿç‡é™åˆ¶ï¼Œç­‰å¾…åé‡è¯•
                error_info = response.json() if response.content else {}
                wait_time = 2  # é»˜è®¤ç­‰å¾…2ç§’
                print(f"[é€šä¹‰åƒé—®API] é€Ÿç‡é™åˆ¶(429)ï¼Œç­‰å¾…{wait_time}ç§’åé‡è¯•...")
                import time
                time.sleep(wait_time)
                # é‡è¯•ä¸€æ¬¡
                retry_response = requests.post(self.tongyi_api_url, headers=headers, json=payload, timeout=30)
                if retry_response.status_code == 200:
                    result = retry_response.json()
                    if result.get('output'):
                        output = result['output']
                        if 'text' in output:
                            generated_text = output['text']
                        elif 'choices' in output and len(output['choices']) > 0:
                            generated_text = output['choices'][0]['message']['content']
                        else:
                            generated_text = None
                        
                        if generated_text:
                            print(f"[é€šä¹‰åƒé—®API] é‡è¯•æˆåŠŸ")
                            return self.parse_ai_result({'generated_text': generated_text}, text)
                print(f"[é€šä¹‰åƒé—®API] é‡è¯•åä»å¤±è´¥ï¼Œè¿”å›Noneä»¥å°è¯•ä¸‹ä¸€ä¸ªAPI")
                return None
            else:
                print(f"[é€šä¹‰åƒé—®API] é”™è¯¯ {response.status_code}: {response.text[:200]}")
                return None
        except Exception as e:
            print(f"[é€šä¹‰åƒé—®API] è°ƒç”¨å¤±è´¥: {e}")
            import traceback
            print(f"[é€šä¹‰åƒé—®API] é”™è¯¯è¯¦æƒ…: {traceback.format_exc()}")
            return None
    
    def _try_huggingface_free(self, prompt, text):
        """å°è¯•ä½¿ç”¨Hugging Faceå…è´¹APIï¼ˆæ— éœ€Tokenï¼‰"""
        for api_url in self.hf_free_api_urls:
            try:
                headers = {"Content-Type": "application/json"}
                payload = {
                    "inputs": prompt,
                    "parameters": {
                        "max_new_tokens": 150,
                        "temperature": 0.3,
                        "return_full_text": False
                    }
                }
                
                print(f"[HF Free API] å°è¯•è°ƒç”¨: {api_url}")
                response = requests.post(api_url, headers=headers, json=payload, timeout=30)
                
                if response.status_code == 200:
                    result = response.json()
                    print(f"[HF Free API] è°ƒç”¨æˆåŠŸ")
                    return self.parse_ai_result(result, text)
                elif response.status_code == 503:
                    error_info = response.json() if response.content else {}
                    estimated_time = error_info.get('estimated_time', 0)
                    print(f"[HF Free API] æ¨¡å‹æ­£åœ¨åŠ è½½ï¼Œé¢„è®¡ç­‰å¾…æ—¶é—´: {estimated_time}ç§’")
                    if estimated_time and estimated_time < 30:
                        import time
                        time.sleep(min(estimated_time + 2, 30))
                        retry_response = requests.post(api_url, headers=headers, json=payload, timeout=30)
                        if retry_response.status_code == 200:
                            return self.parse_ai_result(retry_response.json(), text)
                    continue
                elif response.status_code == 410:
                    print(f"[HF Free API] æ¨¡å‹ä¸å¯ç”¨(410 - Gone)")
                    continue
                elif response.status_code == 429:
                    print(f"[HF Free API] è¯·æ±‚è¿‡å¤š(429)")
                    import time
                    time.sleep(2)
                    continue
                else:
                    print(f"[HF Free API] é”™è¯¯ {response.status_code}: {response.text[:200]}")
                    continue
            except Exception as e:
                print(f"[HF Free API] è°ƒç”¨å¤±è´¥: {e}")
                continue
        return None
    
    def local_analyze(self, text):
        """æœ¬åœ°è§„åˆ™åˆ†æï¼ˆå¤‡ç”¨æ–¹æ¡ˆï¼‰"""
        text_lower = text.lower()
        
        # æƒ…æ„Ÿåˆ†æå…³é”®è¯ï¼ˆæ›´å…¨é¢çš„ä¸­æ–‡å…³é”®è¯ï¼‰
        positive_keywords = ['å¥½', 'æ»¡æ„', 'å–œæ¬¢', 'æ¨è', 'ä¼˜ç§€', 'æ£’', 'èµ', 'ä¸é”™', 'å¾ˆå¥½', 'å®Œç¾', 
                            'èµ', 'ç»™åŠ›', 'å¥½ç”¨', 'æ–¹ä¾¿', 'å¿«æ·', 'æµç•…', 'æ¸…æ™°', 'ç¾è§‚', 'å®ç”¨', 
                            'è´´å¿ƒ', 'ä¸“ä¸š', 'é«˜æ•ˆ', 'ç¨³å®š', 'å¯é ', 'å€¼å¾—', 'è¶…å€¼', 'æƒŠå–œ']
        negative_keywords = ['å·®', 'ä¸å¥½', 'å¤±æœ›', 'é—®é¢˜', 'é”™è¯¯', 'æ…¢', 'å¡', 'å´©æºƒ', 'bug', 'æ•…éšœ',
                            'ç³Ÿç³•', 'åƒåœ¾', 'éš¾ç”¨', 'å¤æ‚', 'éº»çƒ¦', 'å»¶è¿Ÿ', 'å¡é¡¿', 'é—ªé€€', 'æ­»æœº',
                            'ä¸å…¼å®¹', 'ç¼ºå¤±', 'ä¸è¶³', 'ç¼ºé™·', 'æ¼æ´', 'ä¸å®‰å…¨', 'è´µ', 'ä¸å€¼']
        
        positive_count = sum(1 for kw in positive_keywords if kw in text)
        negative_count = sum(1 for kw in negative_keywords if kw in text)
        
        # åˆ¤æ–­æƒ…æ„Ÿ
        if positive_count > negative_count and positive_count > 0:
            sentiment = 'æ­£é¢'
        elif negative_count > 0:
            sentiment = 'è´Ÿé¢'
        else:
            sentiment = 'ä¸­æ€§'
        
        # ç®€å•åˆ†ç±»
        summary = self.categorize_text(text)
        
        # æ·»åŠ ç®€å•è¡¨æƒ…
        sentiment_emoji = {
            'æ­£é¢': 'æ­£é¢ğŸ˜Š',
            'è´Ÿé¢': 'è´Ÿé¢ğŸ˜ ',
            'ä¸­æ€§': 'ä¸­æ€§ğŸ˜'
        }
        
        return [{
            'sentiment': sentiment_emoji.get(sentiment, sentiment),
            'summary': summary,
            'snippet': text,
            'confidence': 0.7
        }]
    
    def categorize_text(self, text):
        """ç®€å•çš„æ–‡æœ¬åˆ†ç±»"""
        text_lower = text.lower()
        
        categories = {
            'åŠŸèƒ½ - Bug/ç¨³å®šæ€§': ['åŠŸèƒ½', 'ä¸èƒ½', 'æ— æ³•', 'ä¸æ”¯æŒ', 'ç¼ºå°‘', 'æ²¡æœ‰', 'ç¼ºå¤±', 'ä¸å®Œå–„', 'ä¸å®Œæ•´', 'æ­»æœº', 'æŠ¥é”™', 'å¤±æ•ˆ', 'ä¸æ˜¾ç¤º'],
            'åŠŸèƒ½ - çµæ´»æ€§/é…ç½®èƒ½åŠ›': ['è‡ªå®šä¹‰', 'é…ç½®', 'é€‰é¡¹', 'çµæ´»', 'æ›´å¤šåŠŸèƒ½', 'æ”¯æŒ', 'è®¾ç½®'],
            'åŠŸèƒ½ - å®ç”¨æ€§/å®Œæ•´åº¦': ['åŠæˆå“', 'ä¸å¥½ç”¨', 'é¸¡è‚‹', 'æ²¡ç”¨', 'å¥‡æ€ª'],
            'ä½“éªŒ - æ“ä½œå¤æ‚åº¦': ['éš¾æ‰¾', 'æ­¥éª¤', 'å¤æ‚', 'éº»çƒ¦', 'é€»è¾‘', 'åäººç±»', 'éš¾ç”¨'],
            'ä½“éªŒ - æ€§èƒ½/åŠ è½½é€Ÿåº¦': ['æ…¢', 'å¡', 'å»¶è¿Ÿ', 'åŠ è½½', 'å“åº”', 'å¡é¡¿', 'é€Ÿåº¦', 'æ€§èƒ½', 'ä¼˜åŒ–'],
            'èµ„æº - æ¨¡æ¿ä¸°å¯Œåº¦': ['æ¨¡æ¿', 'é£æ ¼', 'ä¸»é¢˜', 'æ ·å¼'],
            'èµ„æº - æ’ä»¶ç”Ÿæ€': ['æ’ä»¶', 'æ‰©å±•', 'åº”ç”¨'],
            'æœåŠ¡ - å¸®åŠ©ä¸å¼•å¯¼': ['æ–‡æ¡£', 'æ•™ç¨‹', 'æŒ‡å¼•', 'è¯´æ˜', 'å¸®åŠ©', 'å®¢æœ', 'æ”¯æŒ'],
        }
        
        # è®¡ç®—æ¯ä¸ªç±»åˆ«çš„åŒ¹é…åˆ†æ•°
        category_scores = {}
        for category, keywords in categories.items():
            score = sum(1 for kw in keywords if kw in text)
            if score > 0:
                category_scores[category] = score
        
        # è¿”å›å¾—åˆ†æœ€é«˜çš„ç±»åˆ«
        if category_scores:
            return max(category_scores.items(), key=lambda x: x[1])[0]
        
        return 'å…¶ä»–é—®é¢˜'
    
    def parse_ai_result(self, result, text):
        """è§£æAIè¿”å›çš„JSONç»“æœ"""
        import json
        try:
            generated_text = ""
            # è·å–ç”Ÿæˆçš„æ–‡æœ¬
            if isinstance(result, dict):
                if 'generated_text' in result:
                    generated_text = result['generated_text']
                elif 'text' in result:
                     generated_text = result['text']
            elif isinstance(result, str):
                generated_text = result

            if not generated_text:
                return None

            # å°è¯•è§£æJSON
            # æ¸…ç†å¯èƒ½çš„markdownæ ‡è®°
            clean_text = generated_text.strip()
            if clean_text.startswith('```json'):
                clean_text = clean_text[7:]
            if clean_text.startswith('```'):
                clean_text = clean_text[3:]
            if clean_text.endswith('```'):
                clean_text = clean_text[:-3]
            clean_text = clean_text.strip()
            
            # æ‰¾åˆ°JSONæ•°ç»„éƒ¨åˆ†
            start = clean_text.find('[')
            end = clean_text.rfind(']') + 1
            
            parsed = None
            if start != -1 and end != -1:
                json_str = clean_text[start:end]
                try:
                    parsed = json.loads(json_str)
                except:
                    pass
            
            # å¦‚æœæ²¡æ‰¾åˆ°æ•°ç»„ï¼Œå°è¯•è§£ææ•´ä¸ªæ–‡æœ¬ä¸ºå¯¹è±¡
            if not parsed:
                 try:
                    parsed = json.loads(clean_text)
                    if isinstance(parsed, dict):
                        parsed = [parsed]
                 except:
                    pass

            if not parsed:
                return None
                
            validated_results = []
            for item in parsed:
                sentiment = item.get('sentiment', 'ä¸­æ€§ğŸ˜')
                # é€‚é…æ–°Promptçš„è¿”å›å­—æ®µ 'category'
                summary = item.get('category') or item.get('summary') or 'å…¶ä»–é—®é¢˜'
                snippet = item.get('snippet', text) 
                
                validated_results.append({
                    'sentiment': sentiment,
                    'summary': summary,
                    'snippet': snippet,
                    'confidence': 0.85
                })
            
            return validated_results

        except Exception as e:
            print(f"[Parse] Error: {str(e)}")
            return None
            
    def analyze_and_categorize(self, rows_data, feedback_col):
        """åˆ†æå¹¶åˆ†ç±»æ•°æ®ï¼ˆæ”¯æŒå¤šè§‚ç‚¹æ‹†åˆ†ï¼‰"""
        print(f"[Analyze] Analyzing {len(rows_data)} rows...")
        
        # æ‰å¹³åŒ–çš„æ‰€æœ‰æ„è§åˆ—è¡¨ï¼ŒåŒ…å« row_id ç”¨äºè®¡ç®—ç”¨æˆ·æ•°
        all_opinions = []
        
        total_rows = len(rows_data)
        if hasattr(self, 'progress_callback') and self.progress_callback:
            self.progress_callback(0, total_rows, f'å¼€å§‹åˆ†æï¼Œå…± {total_rows} æ¡åé¦ˆ...')
            
        for idx, row_info in enumerate(rows_data, 1):
            if self.stop_flag and self.stop_flag.is_set():
                raise KeyboardInterrupt("åˆ†æè¢«ç”¨æˆ·ç»ˆæ­¢")
                
            if hasattr(self, 'progress_callback') and self.progress_callback:
                self.progress_callback(idx, total_rows, f'æ­£åœ¨åˆ†æç¬¬ {idx}/{total_rows} æ¡åé¦ˆ...')
            
            # AI åˆ†æè¿”å›åˆ—è¡¨
            analysis_list = self.analyze_with_ai(row_info[feedback_col])
            
            # API å»¶è¿Ÿ
            if self.tongyi_key and idx < total_rows:
                import time
                time.sleep(0.3)
            
            # æ‰å¹³åŒ–å­˜å‚¨ (ä¸æ‹†åˆ†ï¼Œç›´æ¥å­˜)
            # å…¼å®¹è¿”å›åˆ—è¡¨çš„æƒ…å†µï¼ˆå¦‚æœæœ‰ï¼‰
            first_opinion = analysis_list[0] if analysis_list and len(analysis_list) > 0 else {
                'summary': 'å…¶ä»–é—®é¢˜', 'sentiment': 'ä¸­æ€§ğŸ˜'
            }

            
            all_opinions.append({
                'row_id': idx,
                'summary': first_opinion['summary'],
                'sentiment': first_opinion['sentiment'],
                'snippet': row_info[feedback_col], # snippetç›´æ¥ç­‰äºå…¨æ–‡
                'full_feedback': row_info[feedback_col],
                'row_data': row_info
            })
                
        return all_opinions

    def generate_analysis_sheet(self, all_opinions, total_users, sheet_name, sort_by='user', original_columns=None):
        """ç”Ÿæˆå½’ç±»åçš„åˆ†æSheet (åŒ…å«åŸå§‹åˆ—)
        - å°†åŒç±»VOCè¡Œæ”¾åœ¨ä¸€èµ·ï¼Œå¹¶ä¸ºåˆ†ç»„åˆ›å»ºåˆå¹¶çš„æ€»é—®é¢˜æ ‡é¢˜
        - åŠŸèƒ½/ä½“éªŒç­‰åˆ†ç±»å•ç‹¬æ”¾ä¸€åˆ—ï¼Œä¸ä¸é—®é¢˜æ ‡é¢˜æ··åœ¨ä¸€èµ·
        """
        if original_columns is None:
            original_columns = []

        # å…ˆæŒ‰â€œé—®é¢˜æ ‡é¢˜â€åˆ†ç»„ï¼Œä¿æŒå‡ºç°é¡ºåº
        grouped = []
        group_map = {}

        def split_summary(summary_text):
            """æ‹†åˆ†åˆ†ç±»ï¼šå‰åŠéƒ¨åˆ†ä¸ºå½’ç±»ï¼ˆåŠŸèƒ½/ä½“éªŒï¼‰ï¼ŒååŠéƒ¨åˆ†ä¸ºæ€»é—®é¢˜æ ‡é¢˜"""
            if not summary_text:
                return "å…¶ä»–é—®é¢˜", ""
            text = str(summary_text).strip()
            parts = [p.strip() for p in re.split(r'[-â€”]', text, maxsplit=1)]
            if len(parts) == 2 and parts[0] and parts[1]:
                return parts[1], parts[0]  # (é—®é¢˜æ ‡é¢˜, é—®é¢˜å½’ç±»)
            return text, ""  # æ²¡æœ‰æ˜ç¡®å½’ç±»æ—¶ï¼Œæ•´å¥ä½œä¸ºæ ‡é¢˜

        for opinion in all_opinions:
            title, category = split_summary(opinion.get('summary'))
            key = (title, category)
            if key not in group_map:
                group_map[key] = []
                grouped.append((key, group_map[key]))
            group_map[key].append({**opinion, 'title': title, 'category': category})

        # æ„å»ºSheet Data
        celldata = []

        # è¡¨å¤´: [é—®é¢˜æ€»æ ‡é¢˜, é—®é¢˜å½’ç±», ç”¨æˆ·æƒ…ç»ª] + åŸå§‹åˆ—
        headers = ['é—®é¢˜æ€»æ ‡é¢˜', 'é—®é¢˜å½’ç±»', 'ç”¨æˆ·æƒ…ç»ª'] + original_columns

        for col_idx, header in enumerate(headers):
            celldata.append({
                'r': 0,
                'c': col_idx,
                'v': {
                    'v': header,
                    'm': header,
                    'ct': {'fa': 'General', 't': 'g'},
                    'bg': '#EDEBE9',
                    'bl': 1
                }
            })

        current_row = 1
        config = {'merge': {}, 'columnlen': {}}

        # æŒ‰åˆ†ç»„å¡«å……æ•°æ®ï¼Œå¹¶å¯¹åˆ†ç»„åˆ—åšåˆå¹¶
        for (title, category), opinions in grouped:
            start_row = current_row
            group_rows = len(opinions)

            for opinion in opinions:
                row_idx = current_row

                # é—®é¢˜æ€»æ ‡é¢˜ & é—®é¢˜å½’ç±» & ç”¨æˆ·æƒ…ç»ªï¼ˆåªåœ¨ç»„é¦–ç”Ÿæˆï¼Œä¹‹åä¾èµ–åˆå¹¶ï¼‰
                if row_idx == start_row:
                    celldata.append({
                        'r': row_idx,
                        'c': 0,
                        'v': {
                            'v': title,
                            'm': title,
                            'ct': {'fa': 'General', 't': 'g'},
                            'vt': 1,
                            'ht': 1,
                            'bg': '#F6F8FA'
                        }
                    })

                    celldata.append({
                        'r': row_idx,
                        'c': 1,
                        'v': {
                            'v': category or 'æœªåˆ†ç±»',
                            'm': category or 'æœªåˆ†ç±»',
                            'ct': {'fa': 'General', 't': 'g'},
                            'vt': 1,
                            'ht': 1,
                            'bg': '#F6F8FA'
                        }
                    })

                    font_color = '#000000'
                    if 'è´Ÿé¢' in str(opinion['sentiment']):
                        font_color = '#FF0000'
                    elif 'æ­£é¢' in str(opinion['sentiment']):
                        font_color = '#008000'

                    celldata.append({
                        'r': row_idx,
                        'c': 2,
                        'v': {
                            'v': opinion['sentiment'],
                            'm': opinion['sentiment'],
                            'ct': {'fa': 'General', 't': 'g'},
                            'fc': font_color,
                            'vt': 1,
                            'ht': 1
                        }
                    })

                # åŸå§‹åˆ—æ•°æ®ï¼ˆä»åˆ—3å¼€å§‹ï¼‰
                for col_i, col_name in enumerate(original_columns):
                    val = opinion['row_data'].get(col_name, '')
                    if isinstance(val, float):
                        import math
                        if math.isnan(val) or math.isinf(val):
                            val = ""
                    val_str = str(val)

                    celldata.append({
                        'r': row_idx,
                        'c': 3 + col_i,
                        'v': {
                            'v': val_str,
                            'm': val_str,
                            'ct': {'fa': 'General', 't': 'g'}
                        }
                    })

                current_row += 1

            # ç”Ÿæˆåˆå¹¶é…ç½®ï¼ˆå°†åŒç»„çš„â€œé—®é¢˜æ€»æ ‡é¢˜â€â€œé—®é¢˜å½’ç±»â€â€œç”¨æˆ·æƒ…ç»ªâ€åˆ—åˆå¹¶ï¼‰
            if group_rows > 1:
                for col_idx in (0, 1, 2):
                    config['merge'][f"{start_row}_{col_idx}"] = {
                        "r": start_row,
                        "c": col_idx,
                        "rs": group_rows,
                        "cs": 1
                    }

        # åˆ—å®½é…ç½®
        column_len = {
            '0': 220,  # é—®é¢˜æ€»æ ‡é¢˜
            '1': 120,  # é—®é¢˜å½’ç±»ï¼ˆåŠŸèƒ½/ä½“éªŒç­‰ï¼‰
            '2': 100,  # ç”¨æˆ·æƒ…ç»ª
        }

        return {
            'name': sheet_name,
            'celldata': celldata,
            'config': {
                'merge': config.get('merge', {}),
                'columnlen': column_len
            }
        }

    def create_sheet_data(self, ws, sheet_name, sheet_idx):
        """å°†Worksheetè½¬æ¢ä¸ºLuckysheetæ ¼å¼çš„æ•°æ®"""
        celldata = []
        max_row = ws.max_row
        max_col = ws.max_column
        
        # è¯»å–æ‰€æœ‰å•å…ƒæ ¼
        for row in range(1, max_row + 1):
            for col in range(1, max_col + 1):
                cell = ws.cell(row=row, column=col)
                if cell.value is not None:
                    # å¤„ç†å„ç§ç±»å‹çš„å€¼ï¼Œç¡®ä¿ä¸ä¼šäº§ç”ŸNaN
                    try:
                        raw_value = cell.value
                        cell_value = str(raw_value)
                        
                        # æ£€æŸ¥ç‰¹æ®Šå€¼
                        lower_val = cell_value.lower()
                        if lower_val == 'nan' or lower_val == 'inf' or lower_val == '-inf':
                            cell_value = ""
                    except Exception:
                        cell_value = ""
                    
                    celldata.append({
                        "r": row - 1,
                        "c": col - 1,
                        "v": {
                            "v": cell_value,
                            "m": cell_value,
                            "ct": {"fa": "General", "t": "g"}
                        }
                    })
        
        return {
            "name": sheet_name,
            "index": str(sheet_idx),
            "order": sheet_idx,
            "status": 1 if sheet_idx == 0 else 0,
            "celldata": celldata
        }

    def analyze_dataframe(self, df, original_sheet_data=None):
        """åˆ†æDataFrameçš„æ ¸å¿ƒé€»è¾‘
        
        Args:
            df: pandas DataFrame containing the data to analyze
            original_sheet_data: Optional dict for original sheet (if None, will be generated from df)
        
        Returns:
            list of sheet data dicts
        """
        try:
            columns = df.columns.tolist()
            
            # æ™ºèƒ½è¯†åˆ«åé¦ˆåˆ—
            feedback_col = None
            
            # 1. å…³é”®è¯åŒ¹é… (ä¼˜å…ˆçº§æœ€é«˜)
            keywords = ['feedback', 'comment', 'content', 'voice', 'opinion', 'å»ºè®®', 'åé¦ˆ', 'æ„è§', 'åŸå£°', 'å†…å®¹', 'è¯„ä»·']
            for col in columns:
                if any(k in str(col).lower() for k in keywords):
                    feedback_col = col
                    print(f"[Analyze] Automatically detected feedback column by keyword: {feedback_col}")
                    break
            
            # 2. å¦‚æœæ²¡æ‰¾åˆ°ï¼Œä½¿ç”¨å†…å®¹å¹³å‡é•¿åº¦åˆ¤æ–­
            if not feedback_col:
                max_avg_len = 0
                best_col = columns[0]
                
                for col in columns:
                    sample_texts = [str(x) for x in df[col].head(10).tolist() if pd.notna(x)]
                    if not sample_texts:
                        continue
                        
                    avg_len = sum(len(t) for t in sample_texts) / len(sample_texts)
                    
                    if avg_len > max_avg_len:
                        max_avg_len = avg_len
                        best_col = col
                
                feedback_col = best_col
                print(f"[Analyze] Automatically detected feedback column by length: {feedback_col} (Avg Len: {max_avg_len:.1f})")

            print(f"[Analyze] Using column '{feedback_col}' as feedback source.")
            rows = df.to_dict('records')
            total_users = len(rows)
            
            # åˆ†æå¹¶è·å–æ‰å¹³åŒ–æ•°æ®
            all_opinions = self.analyze_and_categorize(rows, feedback_col)
            
            sheets_data = []
            
            # æ·»åŠ åŸå§‹æ•°æ®Sheet
            if original_sheet_data:
                sheets_data.append(original_sheet_data)
            else:
                # ä»DataFrameç”ŸæˆåŸå§‹æ•°æ®sheet
                original_sheet = self._dataframe_to_sheet_data(df, "åŸå§‹æ•°æ®", 0)
                sheets_data.append(original_sheet)

            # ç”Ÿæˆåˆ†æç»“æœ Sheet
            sheet_user = self.generate_analysis_sheet(all_opinions, total_users, "åˆ†æç»“æœ", 'user', original_columns=columns)
            sheet_user['index'] = 1
            sheet_user['order'] = 1
            sheet_user['status'] = 1
            sheets_data.append(sheet_user)
            
            return sheets_data
            
        except Exception as e:
            print(f"[Analyze] Error: {str(e)}")
            import traceback
            traceback.print_exc()
            return []
    
    def _dataframe_to_sheet_data(self, df, sheet_name, sheet_idx):
        """å°†DataFrameè½¬æ¢ä¸ºsheet dataæ ¼å¼"""
        celldata = []
        
        # è¡¨å¤´
        for col_idx, col_name in enumerate(df.columns):
            celldata.append({
                'r': 0,
                'c': col_idx,
                'v': {
                    'v': str(col_name),
                    'm': str(col_name),
                    'ct': {'fa': 'General', 't': 'g'}
                }
            })
        
        # æ•°æ®è¡Œ
        for row_idx, row in df.iterrows():
            for col_idx, col_name in enumerate(df.columns):
                val = row[col_name]
                if pd.notna(val):
                    val_str = str(val)
                    celldata.append({
                        'r': row_idx + 1,
                        'c': col_idx,
                        'v': {
                            'v': val_str,
                            'm': val_str,
                            'ct': {'fa': 'General', 't': 'g'}
                        }
                    })
        
        return {
            'name': sheet_name,
            'index': str(sheet_idx),
            'order': sheet_idx,
            'status': 1 if sheet_idx == 0 else 0,
            'celldata': celldata
        }

    def analyze_file(self, filepath):
        """åˆ†ææ–‡ä»¶çš„ä¸»å…¥å£"""
        try:
            print(f"[Analyze] Reading file: {filepath}")
            if filepath.endswith('.csv'):
                df = pd.read_csv(filepath)
            else:
                df = pd.read_excel(filepath)
            
            # ä¸ºäº†ä¿æŒå…¼å®¹æ€§ï¼Œä½¿ç”¨ openpyxl è¯»å–ç”ŸæˆåŸå§‹ sheet data
            import openpyxl
            wb = openpyxl.load_workbook(filepath)
            ws = wb.active
            original_sheet = self.create_sheet_data(ws, "åŸå§‹æ•°æ®", 0)
            
            # è°ƒç”¨æ ¸å¿ƒåˆ†æé€»è¾‘
            return self.analyze_dataframe(df, original_sheet_data=original_sheet)
            
        except Exception as e:
            print(f"[Analyze] Error: {str(e)}")
            import traceback
            traceback.print_exc()
            return []
